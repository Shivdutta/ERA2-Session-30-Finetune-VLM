{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1235bafd-9625-44da-88cd-8c347abce2d5",
      "metadata": {
        "id": "1235bafd-9625-44da-88cd-8c347abce2d5"
      },
      "outputs": [],
      "source": [
        "!pip install glob2 peft wandb datasets trl==0.8.5 transformers accelerate -q\n",
        "!pip install -U bitsandbytes flash_attn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d74dae-af88-40d4-9b17-da067eaabff8",
      "metadata": {
        "id": "22d74dae-af88-40d4-9b17-da067eaabff8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPVisionModel, AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "import gc\n",
        "import numpy as np\n",
        "import os\n",
        "import glob2\n",
        "from dataset import collate_fn, llavadataset\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import pickle\n",
        "\n",
        "# import wandb\n",
        "# from google.colab import userdata\n",
        "# wandb1 = userdata.get('wandb')\n",
        "# os.environ[\"WANDB_API_KEY\"] = wandb1\n",
        "os.environ[\"WANDB_API_KEY\"] = \"ABABABABABABABA\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/Colab_Notebooks/Session30"
      ],
      "metadata": {
        "id": "EIuJNdrJEu7j"
      },
      "id": "EIuJNdrJEu7j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# teacher forcing simulated annealing scheduler\n",
        "# Below code is used simulated annealing scheduler called frange_cycle_linear.\n",
        "# This function generates a cyclic schedule for a hyperparameter, often used in teacher forcing or other training techniques where a parameter\n",
        "# (such as the probability of applying teacher forcing) needs to change gradually over time.\n",
        "\n",
        "# The frange_cycle_linear function is commonly used to schedule the teacher forcing ratio during training, which might start at a low value\n",
        "# (e.g., almost always letting the model predict on its own) and gradually increase (i.e., using ground truth more often).\n",
        "# With multiple cycles, this ratio oscillates throughout the training, allowing the model to learn in different regimes over time.\n",
        "\n",
        "def frange_cycle_linear(n_iter, start=0.0001, stop=0.9999,  n_cycle=1, ratio=0.8):\n",
        "    # n_iter : total number of iterations for which the schedule will be computed.\n",
        "    # start : initial value of the schedule. For teacher forcing, this could represent the starting probability of forcing the model to use the ground-truth data during training.\n",
        "    # stop : maximum or final value of the schedule, typically representing the probability of not using teacher forcing.\n",
        "    # n_cycle : means how many times the schedule will oscillate from start to stop.\n",
        "    # ratio :  fraction of each cycle where the parameter linearly increases from start to stop.\n",
        "\n",
        "    # Create  Schedule Array:\n",
        "    # This initializes an array L of length n_iter with all elements set to stop. This means that if no further changes are made, the parameter will stay at the stop value throughout the training.\n",
        "    L = np.ones(n_iter) * stop\n",
        "\n",
        "    # period: Defines the length of each cycle, i.e., how many iterations each cycle spans. If n_cycle=1, the entire schedule is a single cycle; if n_cycle=2, the period is half the total iterations.\n",
        "    # step: This defines the amount by which the parameter will increase in each iteration during the linear growth phase of the cycle.\n",
        "    # The ratio controls how much of the period is used for this linear increase.\n",
        "    period = n_iter/n_cycle\n",
        "    step = (stop-start)/(period*ratio) # linear schedule\n",
        "\n",
        "    # runs for each cycle.\n",
        "    # This loop ensures that the parameter starts at start, increases linearly over part of the cycle (determined by ratio), and then stays at stop for the remainder of the cycle.\n",
        "    for c in range(n_cycle):\n",
        "        # For each cycle, start the parameter value (v) at start and initialize an index (i) to 0.\n",
        "        v, i = start, 0\n",
        "        #  For each cycle, increment the value v by the step size and place it into the correct index of the array L.\n",
        "        while v <= stop and (int(i+c*period) < n_iter):\n",
        "            # Update the schedule at the correct position in L.\n",
        "            L[int(i+c*period)] = v\n",
        "            # The value increases linearly from start towards stop.\n",
        "            v += step\n",
        "            i += 1\n",
        "    S\n",
        "    # After constructing the schedule in L, the function returns 1 - L.\n",
        "    # This effectively inverts the values in L, making the schedule start from (1 - stop) and end at (1 - start).\n",
        "    # This is often done to control the probability of certain actions, such as teacher forcing.\n",
        "    return (1 - L)"
      ],
      "metadata": {
        "id": "Tdy92wAEE9pm"
      },
      "id": "Tdy92wAEE9pm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define models\n",
        "phi_model_name  = \"microsoft/phi-2\"\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "device = 'cuda'\n",
        "max_steps = 100000"
      ],
      "metadata": {
        "id": "L27tf0djE9uv"
      },
      "id": "L27tf0djE9uv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annealing_teacher_forcing_scheduler = frange_cycle_linear(max_steps)\n",
        "\n",
        "class SimpleResBlock(nn.Module):\n",
        "    def __init__(self, phi_embed):\n",
        "        super().__init__()\n",
        "        # Layer Normalization: Normalizes the input to have zero mean and unit variance across the feature dimension.\n",
        "        # It helps stabilize the training by ensuring that the values passed through the network remain in a consistent range.\n",
        "        self.pre_norm = nn.LayerNorm(phi_embed)\n",
        "\n",
        "        # nn.Sequential defines a sequential container, meaning a series of layers applied one after another.\n",
        "        # nn.Linear(phi_embed, phi_embed): A fully connected (linear) layer that takes an input of size phi_embed and outputs the same size (phi_embed).\n",
        "        # GELU (Gaussian Error Linear Unit) is an activation function that smooths out nonlinearities.\n",
        "        # It's similar to ReLU, but it has a smoother gradient, which can improve performance in some architectures.\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(phi_embed, phi_embed),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(phi_embed, phi_embed)\n",
        "        )\n",
        "    # This method defines how the input x passes through the block during forward propagation.\n",
        "    # It takes an input tensor x, normalizes it, processes it through two linear layers with a GELU activation in between, and then adds the original input x back to the output of the transformation.\n",
        "    # The residual connection is important because it allows the model to retain information from earlier layers and makes it easier to train deep models.\n",
        "    def forward(self, x):\n",
        "        x = self.pre_norm(x)\n",
        "        # key part of a residual block\n",
        "        return x + self.proj(x)"
      ],
      "metadata": {
        "id": "R_TvyC9GE911"
      },
      "id": "R_TvyC9GE911",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code defines a neural network model called CLIPPhi2Model,\n",
        "# which combines two pretrained models:\n",
        "# a CLIP vision model for image embeddings\n",
        "# and a causal language model (Phi-2)\n",
        "# for text generation.\n",
        "class CLIPPhi2Model(torch.nn.Module):\n",
        "    # size of the image embeddings from the CLIP model (768-dimensional).\n",
        "    # size of the text embeddings from the Phi-2 model (2560-dimensional).\n",
        "    def __init__(self, clip_embed=768, phi_embed=2560):\n",
        "        super().__init__()\n",
        "\n",
        "        #  End-of-sequence token ID for the language model.\n",
        "        self.EOS_TOKEN_ID    = 50256\n",
        "        # token to represent the presence of an image.\n",
        "        self.IMAGE_TOKEN_ID  = 23893 # token for comment\n",
        "\n",
        "        # pretrained models\n",
        "        self.phi_model = AutoModelForCausalLM.from_pretrained(phi_model_name,\n",
        "                                            torch_dtype=torch.float16,\n",
        "                                            trust_remote_code=True)\n",
        "        self.clip_model = CLIPVisionModel.from_pretrained(clip_model_name)\n",
        "\n",
        "        # projection layers\n",
        "        #  A linear layer that projects the lower-dimensional image embeddings (768 from CLIP) to match the higher-dimensional text embeddings (2560 from Phi-2).\n",
        "        self.projection = torch.nn.Linear(clip_embed, phi_embed)\n",
        "        # A simple residual block (SimpleResBlock) that processes the projected image embeddings.\n",
        "        self.resblock = SimpleResBlock(phi_embed)\n",
        "\n",
        "        # Freeze Weights\n",
        "        # IMPORTANT:\n",
        "        # The pretrained models (both Phi-2 and CLIP) are frozen so their weights are not updated during training.\n",
        "        # This is common in transfer learning to use pretrained features while only training new layers.\n",
        "        for network in [self.phi_model, self.clip_model]:\n",
        "            for param in network.parameters():\n",
        "                param.requires_grad_(False)\n",
        "\n",
        "        # load checkpoint weights\n",
        "        # If pre-trained projection and residual block weights are available (clipphi_proj.pth and clipphi_resblock.pth), they are loaded.\n",
        "        if os.path.isfile('model_chkpt/clipphi_proj.pth'):\n",
        "            self.projection.load_state_dict(torch.load('model_chkpt/clipphi_proj.pth'))\n",
        "            self.resblock.load_state_dict(torch.load('model_chkpt/clipphi_resblock.pth'))\n",
        "\n",
        "\n",
        "    # To generate text (e.g., captions) from images using the model.\n",
        "    # images: The processed input image data. max_length: Maximum length of the generated text.  tokenizer: Tokenizer to convert text into token IDs and vice versa.\n",
        "    def generate(self,images,max_length,tokenizer):\n",
        "        # clip model output for image\n",
        "        # input images are passed through the CLIP model (clip_model) to get their embeddings.\n",
        "        clip_outputs = self.clip_model(**images)\n",
        "        # remove cls token\n",
        "        images = clip_outputs.last_hidden_state[:,1:,:]\n",
        "        # The CLIP image embeddings are projected to the same dimensionality as the Phi-2 embeddings (2560) using self.projection\n",
        "        image_embeds = self.projection(images)\n",
        "        # This is further processed by the residual block (self.resblock).\n",
        "        image_embeds = self.resblock(image_embeds).to(torch.float16)\n",
        "\n",
        "        # Batch Size: Extract the number of images (batch_size).\n",
        "        batch_size = images.size(0)\n",
        "        # predicted_caption: Initialize a tensor to hold the generated caption tokens. It’s filled with the EOS token (50256), which represents the end-of-sequence.\n",
        "        predicted_caption = torch.full((batch_size,max_length),50256)\n",
        "        # Image Token Embedding: A custom token representing the image is embedded using the Phi-2 model's token embeddings.\n",
        "        img_token_tensor = torch.tensor(self.IMAGE_TOKEN_ID).repeat(batch_size, 1)\n",
        "        # This acts as the start of the sequence (bos_token) for generation.\n",
        "        bos_token_embeds = self.phi_model.model.embed_tokens(img_token_tensor.to(image_embeds.device))\n",
        "        # Concatenate: The image embeddings (image_embeds) are concatenated with the BOS token embeddings (bos_token_embeds) to create the initial input for the text generation process.\n",
        "        combined_embeds  = torch.cat([image_embeds, bos_token_embeds], dim=1) # 4,9,2560\n",
        "\n",
        "\n",
        "        Prepare for the Next Token: The predicted token is embedded using the Phi-2 model's embeddings (embed_tokens) and concatenated with the existing sequence of embeddings (combined_embeds). This updated embedding sequence is then used in the next iteration to predict the next token.\n",
        "\n",
        "        Repeat: The process repeats for the entire caption length.\n",
        "        # Loop through max_length: For each position pos (up to max_length - 1), the model generates one token at a time.\n",
        "        for pos in range(max_length - 1):\n",
        "            # pass through the model\n",
        "\n",
        "            # Model Forward Pass: The Phi-2 language model takes the combined_embeds (which includes both the image and any previously generated tokens) as input and predicts the next token's logits.\n",
        "            model_output_logits = self.phi_model.forward(inputs_embeds = combined_embeds)['logits'] # 4,49,51200\n",
        "            predicted_word_token_logits = model_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
        "            # Token Prediction: The logits for the last position ([:, -1, :]) are extracted and passed through\n",
        "            # torch.argmax to get the predicted token with the highest probability.\n",
        "            # This token is stored in the predicted_caption tensor.\n",
        "            predicted_word_token = torch.argmax(predicted_word_token_logits, dim = -1) # 4,1\n",
        "            predicted_caption[:,pos] = predicted_word_token.view(1,-1).to('cpu')\n",
        "            # The predicted token is embedded using the Phi-2 model's embeddings (embed_tokens).\n",
        "            next_token_embeds = self.phi_model.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
        "            # This is concatenated with the existing sequence of embeddings (combined_embeds).\n",
        "            combined_embeds   = torch.cat([combined_embeds, next_token_embeds], dim=1)\n",
        "            # This updated embedding sequence is then used in the next iteration to predict the next token.\n",
        "\n",
        "        # After generating the tokens for all positions, the method returns the complete predicted_caption tensor containing the token IDs for the generated caption.\n",
        "        return predicted_caption\n",
        "\n",
        "    def forward(self, images, target_captions,step,max_steps):\n",
        "        # batch_size: The number of samples in the batch.\n",
        "        # target_length: The length of the target captions (number of tokens).\n",
        "        batch_size    = target_captions.size(0)\n",
        "        target_length = target_captions.shape[1]\n",
        "         #print(f\"GPU memory {torch.cuda.max_memory_allocated()/ (1024 ** 3):.2f} GB\")\n",
        "\n",
        "        # clip model output for image\n",
        "        # Input: The input images are passed through the CLIP model, which outputs image embeddings.\n",
        "        clip_outputs = self.clip_model(**images)\n",
        "        # Remove CLS Token: The CLS token (used for classification) is removed, as it’s not needed for caption generation.\n",
        "        images = clip_outputs.last_hidden_state[:,1:,:] # remove cls token\n",
        "\n",
        "        # projection layer\n",
        "        # The 768-dimensional CLIP image embeddings are projected to the 2560-dimensional space required by the Phi-2 model using a linear layer.\n",
        "        image_embeds = self.projection(images).to(torch.float16)\n",
        "        #image_embeds = self.resblock(image_embeds).to(torch.float16)\n",
        "\n",
        "        # add comment token from phi2\n",
        "        # A special image token (self.IMAGE_TOKEN_ID) is embedded using the Phi-2 model's embedding layer.\n",
        "        img_token_tensor = torch.tensor(self.IMAGE_TOKEN_ID).repeat(batch_size, 1)\n",
        "        # This token is a placeholder to mark where the image information ends and the text generation starts.\n",
        "        img_token_embeds = self.phi_model.model.embed_tokens(img_token_tensor.to(image_embeds.device))\n",
        "        # The image embeddings are concatenated with the image token embeddings to form combined_embeds, which is the input to the Phi-2 model for text generation.\n",
        "        combined_embeds  = torch.cat([image_embeds, img_token_embeds], dim=1) # 4,49,2560\n",
        "        del clip_outputs\n",
        "        del image_embeds\n",
        "\n",
        "        # for loss\n",
        "        loss = 0\n",
        "        # In each iteration, the Phi-2 model generates one token at a time based on the current input embeddings (combined_embeds), which include both image and previously generated token embeddings.\n",
        "        for pos in range(target_length - 1):\n",
        "            # pass through the model\n",
        "            # The Phi-2 model processes the current embeddings (combined_embeds) to generate the next token's logits (model_output_logits).\n",
        "            model_output_logits = self.phi_model.forward(inputs_embeds = combined_embeds)['logits'] # 4,49,51200\n",
        "            # The logits for the last predicted token in the sequence are extracted ([:, -1, :]), and then reshaped to match the dimensions expected by the loss function.\n",
        "            predicted_word_token_logits = model_output_logits[:, -1, :].unsqueeze(1) # 4,1,51200\n",
        "            # The cross-entropy loss is computed between the predicted token logits and the actual target token at the current position (target_captions[:, pos]).\n",
        "            # The loss is smoothed with label_smoothing to avoid overconfident predictions, and tokens with the EOS_TOKEN_ID are ignored.\n",
        "            pos_loss = F.cross_entropy(predicted_word_token_logits.view(-1,predicted_word_token_logits.size(-1)), target_captions[:, pos].contiguous().view(-1), ignore_index=self.EOS_TOKEN_ID,label_smoothing=0.1)\n",
        "            # print(f\"pos {pos} loss {pos_loss}\")\n",
        "            # The loss for the current token is added to the total batch loss.\n",
        "            loss += pos_loss\n",
        "\n",
        "            # Store Predicted Token: The predicted token is stored in the predicted_caption tensor, which will hold the complete generated sequence.\n",
        "            predicted_word_token = torch.argmax(predicted_word_token_logits, dim=-1) # 4,1\n",
        "            #print(f\"predicted_word_token {predicted_word_token} and target_captions {target_captions[:,pos]}\")\n",
        "            # Teacher Forcing: For the first few tokens (up to pos <= 5) and early in training (step <= int(0.6 * max_steps)),the model uses teacher forcing.\n",
        "            # In this case, instead of relying on its own predictions, the model is fed the correct target token from target_captions.\n",
        "            # do teacher forcing or model output based on annealing scheduler probability\n",
        "            if pos <= 5 and step <= int(0.6 * max_steps): # teacher forcing\n",
        "                next_token_embeds = self.phi_model.model.embed_tokens(target_captions[:,pos].unsqueeze(1)) # 4,1,2560\n",
        "            else:\n",
        "                next_token_embeds = self.phi_model.model.embed_tokens(predicted_word_token) # 4,1,2560\n",
        "\n",
        "            # The predicted token is embedded and concatenated to the existing embeddings to generate the next token in the sequence.\n",
        "            combined_embeds   = torch.cat([combined_embeds, next_token_embeds], dim=1)\n",
        "\n",
        "        #average_loss\n",
        "        # The total loss is averaged over all token positions to get the final loss for the batch.\n",
        "        loss = loss / target_length\n",
        "\n",
        "        # for efficient memory utilization\n",
        "        del combined_embeds\n",
        "        del model_output_logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "zW_qCBb5E95u"
      },
      "id": "zW_qCBb5E95u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This funcation evaluates a trained model on a single batch of data from a validation set.\n",
        "# It compares the model’s predictions (generated text) with the ground truth (target captions) and prints both the target and predicted captions.\n",
        "def model_validate_one_batch(model,device,val_dataloader,max_length,tokenizer):\n",
        "    # This switches the model into evaluation mode, which ensures certain behaviors like dropout and batch normalization are disabled during inference.\n",
        "    model.eval()\n",
        "    # This disables gradient tracking, which reduces memory usage and speeds up computations since gradients are not needed during evaluation or inference.\n",
        "    with torch.no_grad():\n",
        "        # val_dataloader: The validation data loader provides batches of images and their corresponding target captions.\n",
        "        # For each batch, images contains the input images, and target_captions contains the ground-truth captions.\n",
        "        for batch_idx, (images, target_captions) in enumerate(val_dataloader):\n",
        "            images = {'pixel_values': images.to(device)}\n",
        "            target_captions = target_captions.to(device)\n",
        "            # decodes the tokenized target captions back into human-readable text.\n",
        "            # It uses the tokenizer associated with the model to convert the numerical token IDs into their corresponding text.\n",
        "            # The 50256 token ID corresponds to a special token (likely the end-of-sequence or padding token), which is ignored during decoding.\n",
        "            target_captions_decoded = tokenizer.batch_decode(target_captions,ignore_index = 50256)\n",
        "            # This calls the model’s generate method to create text predictions from the input images.\n",
        "            predicted_captions = model.generate(images,max_length,tokenizer)\n",
        "            # The predicted token sequences are also decoded back into human-readable text, just like the target captions.\n",
        "            predicted_captions_decoded = tokenizer.batch_decode(predicted_captions,ignore_index = 50256)\n",
        "\n",
        "            # iterates over the decoded predicted captions and prints both the target and predicted captions for comparison.\n",
        "            # pc_idx: The index of the current caption in the batch.\n",
        "            # target_captions_decoded[pc_idx]: The ground-truth caption for the corresponding image.\n",
        "            # predicted_captions_decoded[pc_idx]: The predicted caption generated by the model for the same image.\n",
        "            for pc_idx,pc in enumerate(predicted_captions_decoded):\n",
        "                print(f\"{pc_idx} - Target captions:\\n {target_captions_decoded[pc_idx]}  \\n{pc_idx} - predicted_captions:\\n {pc} \")\n",
        "            return # validate only 1 batch"
      ],
      "metadata": {
        "id": "sR-WdIEKE-EI"
      },
      "id": "sR-WdIEKE-EI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e79ab0e-7c21-42a4-a121-23f0168a4765",
      "metadata": {
        "id": "4e79ab0e-7c21-42a4-a121-23f0168a4765"
      },
      "outputs": [],
      "source": [
        "# This function, train_model(), is responsible for training the multimodal model that combines image (CLIP) and text (Phi2) embeddings.\n",
        "# The function iteratively processes batches from the training data, calculates the loss, and updates the model parameters.\n",
        "# It also handles periodic model validation, saving checkpoints, and logging progress\n",
        "def train_model(model, train_loader, val_dataloader,optimizer, device,max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer):\n",
        "    print(f\"Training started.\")\n",
        "\n",
        "    # max_step_reached: A flag to track if the maximum number of training steps has been reached.\n",
        "    max_step_reached = 0\n",
        "    # step: Tracks the current step in the training process.\n",
        "    step = 0\n",
        "    # max_length: Sets the maximum length for the generated captions (set to 20 tokens here).\n",
        "    max_length = 20\n",
        "    # running_loss: Accumulates the loss across multiple steps, used for logging the average loss.\n",
        "    running_loss = 0.\n",
        "    # model.train(): Puts the model in training mode (this affects dropout and batch normalization layers, if any).\n",
        "    model.train()\n",
        "\n",
        "    #This outer loop iterates over a large number of epochs\n",
        "    #The inner loop iterates through the train_loader, processing batches of images and their corresponding captions.\n",
        "    # batch_idx: The index of the current batch.\n",
        "    for epoch in range(100000):\n",
        "        for batch_idx, (images, target_captions) in enumerate(train_loader):\n",
        "\n",
        "            # images: The image data is prepared as a dictionary with the key 'pixel_values' and moved to the appropriate device (GPU).\n",
        "            # target_captions: The target captions (text) are also moved to the device.\n",
        "            # manage OOM issue, skip batch for long captions\n",
        "            if target_captions.shape[1] >= max_token_filter:\n",
        "                print(f\"Batch skipped as captions too long.\")\n",
        "                continue\n",
        "            images = {'pixel_values': images.to(device)}\n",
        "            target_captions = target_captions.to(device)\n",
        "\n",
        "            # Clears the gradients before backpropagation (a standard step to prevent accumulating gradients from previous batches).\n",
        "            optimizer.zero_grad()\n",
        "            # The model processes the images and target captions to compute the loss for this batch.\n",
        "            # The step and max_steps parameters may influence the annealing of teacher forcing or other training aspects.\n",
        "            loss = model(images, target_captions,step,max_steps)\n",
        "            #print(f\"teacher {teacher_forcing} and loss {loss}\")\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # log step\n",
        "            if (step % log_step == 0):\n",
        "                if step == 0:\n",
        "                    print(f\"Step {step}/{max_steps}: Avg Running Loss = {running_loss}\")\n",
        "                else:\n",
        "                    print(f\"Step {step}/{max_steps}: Avg Running Loss = {running_loss /log_step}\")\n",
        "                running_loss = 0.\n",
        "            wandb.log({\"step\": step, \"train_loss\": loss.item()})\n",
        "\n",
        "            # increment step\n",
        "            step += 1\n",
        "            teacher_forcing = False\n",
        "\n",
        "            # loss backprop\n",
        "            # loss.backward(): Computes the gradients for all trainable parameters using backpropagation.\n",
        "            # optimizer.step(): Updates the model parameters based on the computed gradients.\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # save model\n",
        "            if step % model_save_step == 0 or (step == max_steps):\n",
        "                print(\"Saving Checkpoint for step : \", step)\n",
        "                torch.save(model.projection.state_dict(),'model_chkpt/clipphi_proj.pth')\n",
        "                torch.save(model.resblock.state_dict(),'model_chkpt/clipphi_resblock.pth')\n",
        "\n",
        "            # check random validation of images\n",
        "            if step % model_val_step == 0 or (step == max_steps):\n",
        "                model_validate_one_batch(model,device,val_dataloader,max_length,tokenizer)\n",
        "                model.train()\n",
        "\n",
        "            # global max steps reached\n",
        "            if step >= max_steps:\n",
        "                max_step_reached = 1\n",
        "                break\n",
        "\n",
        "        if max_step_reached == 1:\n",
        "            break\n",
        "    print(f\"Reached the max steps. Training stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2eaac0-f74d-4db4-bf53-cc25ec2d7c7e",
      "metadata": {
        "id": "2e2eaac0-f74d-4db4-bf53-cc25ec2d7c7e"
      },
      "outputs": [],
      "source": [
        "# defines the workflow for training a multimodal GPT model.\n",
        "# which is likely based on a combination of CLIP and the Phi2 language model.\n",
        "# The code handles loading the data, setting up the model, and defining the training process.\n",
        "def main():\n",
        "    with open(\"captions.pickle\", \"rb\") as fp:   # Unpickling\n",
        "        coco_unpickle = pickle.load(fp)\n",
        "\n",
        "    train_batch_size = 4\n",
        "    val_batch_size   = 2\n",
        "    tokenizer  = AutoTokenizer.from_pretrained(phi_model_name, trust_remote_code=True)\n",
        "\n",
        "    # model\n",
        "    MModalGPT        = CLIPPhi2Model().to(device)\n",
        "    # The maximum number of training steps (iterations) is set to 20,000.\n",
        "    max_steps        = 20000\n",
        "    model_save_step  = 100\n",
        "    model_val_step   = 100\n",
        "    log_step         = 100\n",
        "    # Limits the maximum number of tokens (words, subwords) in the processed inputs, likely to filter out long captions or questions.\n",
        "    max_token_filter = 35\n",
        "\n",
        "    # train_dataloader: A PyTorch DataLoader for the training dataset.\n",
        "    # llavadataset: A custom dataset class that combines the loaded COCO dataset (coco_unpickle), the tokenizer, and other settings. It processes both image and text data.\n",
        "    # collate_fn: A custom function (collate_fn) to pad or batch the input data correctly, including images and tokenized text.\n",
        "    # val_dataloader: The DataLoader for the validation dataset, which is similar to the training DataLoader but with a smaller batch size (val_batch_size=2).\n",
        "\n",
        "    # data loaders\n",
        "    train_dataloader = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'train',tokenizer),\n",
        "                      collate_fn=collate_fn, batch_size=train_batch_size, num_workers = 10, shuffle=True, pin_memory=True)\n",
        "\n",
        "    val_dataloader   = DataLoader(llavadataset(coco_unpickle, phi_model_name,clip_model_name,'val',tokenizer),\n",
        "                      collate_fn=collate_fn, batch_size=val_batch_size, num_workers = 10, shuffle=True, pin_memory=True)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, MModalGPT.parameters()), lr=1e-4)\n",
        "    train_model(MModalGPT, train_dataloader, val_dataloader, optimizer, device, max_steps,model_save_step,model_val_step,log_step,max_token_filter,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fde431c-86b8-4838-bff5-51139d5924ed",
      "metadata": {
        "id": "7fde431c-86b8-4838-bff5-51139d5924ed",
        "outputId": "8a8b08a9-0bf0-410a-c199-6dd0b35dbf70",
        "colab": {
          "referenced_widgets": [
            "10fb542e35dd446687ffed1d62cf3b82"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshivdutta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/work/IG (5087744)/wandb/run-20240929_193101-tem3ldom</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/shivdutta/clip_phi2_project/runs/tem3ldom' target=\"_blank\">clip_phi3_finetune</a></strong> to <a href='https://wandb.ai/shivdutta/clip_phi2_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/shivdutta/clip_phi2_project' target=\"_blank\">https://wandb.ai/shivdutta/clip_phi2_project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/shivdutta/clip_phi2_project/runs/tem3ldom' target=\"_blank\">https://wandb.ai/shivdutta/clip_phi2_project/runs/tem3ldom</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10fb542e35dd446687ffed1d62cf3b82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cudaq/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size 532577 and validation size 59176\n",
            "Train size 532577 and validation size 59176\n",
            "Training started.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0/20000: Avg Running Loss = 7.280514717102051\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  100\n",
            "0 - Target captions:\n",
            " A teddy bear is in a tree with mug on its head.  \n",
            "0 - predicted_captions:\n",
            " ing on the screen.\n",
            "\n",
            "The film was released on DVD on October 1, 2006.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man points to another man with his tongue out.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " ary.\n",
            "\n",
            "The film was shot in the city of Santa Cruz de la Sierra, Bolivia<|endoftext|> \n",
            "Step 100/20000: Avg Running Loss = 7.328832411766053\n",
            "Saving Checkpoint for step :  200\n",
            "0 - Target captions:\n",
            " A batter gets ready to run after hitting a pitch.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " ators_\n",
            "|               <|endoftext|> \n",
            "1 - Target captions:\n",
            " PIECES OF PIE WITH BRIGHT STRAWBERRIES ON TOP.  \n",
            "1 - predicted_captions:\n",
            " :\n",
            " \n",
            "\n",
            "The film is set in a small town in the United States, where<|endoftext|> \n",
            "Step 200/20000: Avg Running Loss = 6.610607056617737\n",
            "Saving Checkpoint for step :  300\n",
            "0 - Target captions:\n",
            " A colorful glass vase sitting on a table.  \n",
            "0 - predicted_captions:\n",
            " : A group of people are sitting on a bench.\n",
            "A group of people are sitting on<|endoftext|> \n",
            "1 - Target captions:\n",
            " a case holding many different pairs of scissors<|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " : A man is sitting on a chair. A woman is sitting on a chair. A man<|endoftext|> \n",
            "Step 300/20000: Avg Running Loss = 6.138972368240356\n",
            "Saving Checkpoint for step :  400\n",
            "0 - Target captions:\n",
            " Small display of oriental chairs and signs next to each other.   \n",
            "0 - predicted_captions:\n",
            " A woman wearing a a white dress and a red scarf is sitting on a chair. A man<|endoftext|> \n",
            "1 - Target captions:\n",
            " Four unopened items for hygiene and for the office supply.<|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman wearing aA woman wearing a red shirt and a black skirt and a white shirt and<|endoftext|> \n",
            "Step 400/20000: Avg Running Loss = 5.918340125083923\n",
            "Saving Checkpoint for step :  500\n",
            "0 - Target captions:\n",
            " a white stove a micro wave and white kitchen cabinets<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " : A woman is sitting on a chair in a living room with a laptop on her lap.<|endoftext|> \n",
            "1 - Target captions:\n",
            " Tennis players at match standing over net shaking hands.  \n",
            "1 - predicted_captions:\n",
            " A woman is a woman wearing a red dress and a white shirt and a black jacket.A<|endoftext|> \n",
            "Step 500/20000: Avg Running Loss = 5.899187684059143\n",
            "Saving Checkpoint for step :  600\n",
            "0 - Target captions:\n",
            " A laptop and a device consisting of a watch and other gears.  \n",
            "0 - predicted_captions:\n",
            " A woman is a woman wearing a red shirt and a black skirt. a black and white.<|endoftext|> \n",
            "1 - Target captions:\n",
            " People sitting at several small tables that have been pulled together.<|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman is a woman wearing a red dress and a white shirt. a white shirt and a<|endoftext|> \n",
            "Step 600/20000: Avg Running Loss = 5.765856890678406\n",
            "Saving Checkpoint for step :  700\n",
            "0 - Target captions:\n",
            " A plate of food with several items and sauce.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman is a woman who is a woman who is a woman who is a woman who is<|endoftext|> \n",
            "1 - Target captions:\n",
            " A building with a clock tower and a lightening rod on top.  \n",
            "1 - predicted_captions:\n",
            " A group of people are walking on the beach in the sand.\n",
            "A group of people are<|endoftext|> \n",
            "Step 700/20000: Avg Running Loss = 5.653788390159607\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  800\n",
            "0 - Target captions:\n",
            " various people standing or sitting on beach holding a sign and kites  \n",
            "0 - predicted_captions:\n",
            " A group of people are walking on the beach. a. Beach. The beach. The beach<|endoftext|> \n",
            "1 - Target captions:\n",
            " A bunch of food that is on a plate.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman is a woman with a baby in her arms. a baby in her arms. a<|endoftext|> \n",
            "Step 800/20000: Avg Running Loss = 5.654556074142456\n",
            "Saving Checkpoint for step :  900\n",
            "0 - Target captions:\n",
            " A teddy bear is wearing a striped scarf.<|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman is a woman in a red dress and a white dress. a red dress and a<|endoftext|> \n",
            "1 - Target captions:\n",
            " Umpire watching a professional baseball game making a safe call.  \n",
            "1 - predicted_captions:\n",
            " A man and a woman walking on a street in a city. a. street. a.<|endoftext|> \n",
            "Step 900/20000: Avg Running Loss = 5.655093772411346\n",
            "Saving Checkpoint for step :  1000\n",
            "0 - Target captions:\n",
            " Glasses of wine sit on a coffee table.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman holding a bottle of water and a bottle of water. a bottle of water and a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A person flying a kite and another person running underneath it on the beach.  \n",
            "1 - predicted_captions:\n",
            " A man and a woman walking on a beach with a dog. a beach. a. beach<|endoftext|> \n",
            "Step 1000/20000: Avg Running Loss = 5.5847353887557984\n",
            "Saving Checkpoint for step :  1100\n",
            "0 - Target captions:\n",
            " A laptop computer on a desk with papers everywhere<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman is a woman who is sitting on a a chair in a room. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " Some girls sitting at a table with some flowers.  \n",
            "1 - predicted_captions:\n",
            " A woman is a woman in a white dress and a white. a white dress and a white<|endoftext|> \n",
            "Step 1100/20000: Avg Running Loss = 5.579158263206482\n",
            "Saving Checkpoint for step :  1200\n",
            "0 - Target captions:\n",
            " A small laptop sitting on a larger laptop on a desk.<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a man standing on a a a. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A vase of pink roses sits between two green vases.  \n",
            "1 - predicted_captions:\n",
            " A woman and a man sitting on a couch. a. a. a. a. a<|endoftext|> \n",
            "Step 1200/20000: Avg Running Loss = 5.579778914451599\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  1300\n",
            "0 - Target captions:\n",
            " The man is crazy about the cooking chicken,  \n",
            "0 - predicted_captions:\n",
            " A woman is a man with a dog on a. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " Two people are playing tennis during the night.  \n",
            "1 - predicted_captions:\n",
            " A group of people playing a game of basketball on a court. a. court. a.<|endoftext|> \n",
            "Step 1300/20000: Avg Running Loss = 5.436848683357239\n",
            "Saving Checkpoint for step :  1400\n",
            "0 - Target captions:\n",
            " South paw baseball player practices his swing with a personalized bat  \n",
            "0 - predicted_captions:\n",
            " A man is a man with a dog on a a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A lady wearing a hat talking on a cell phone.  \n",
            "1 - predicted_captions:\n",
            " A man is a man standing in front of a wall with a picture of a woman on it<|endoftext|> \n",
            "Step 1400/20000: Avg Running Loss = 5.342513189315796\n",
            "Saving Checkpoint for step :  1500\n",
            "0 - Target captions:\n",
            " Profound question on a laptop computer on a messy desk  \n",
            "0 - predicted_captions:\n",
            " A woman is a sitting on a a couch a a a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " Some players in action on the baseball field.<|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is a man walking his dog. a. a. a. a. a.<|endoftext|> \n",
            "Step 1500/20000: Avg Running Loss = 5.491592788696289\n",
            "Saving Checkpoint for step :  1600\n",
            "0 - Target captions:\n",
            " A man at a desk using a computer  \n",
            "0 - predicted_captions:\n",
            " A man is a woman sitting on a a a. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A mixer with ingredients sitting inside of it  \n",
            "1 - predicted_captions:\n",
            " A woman is a a of a a of a of a of a of a of a of<|endoftext|> \n",
            "Step 1600/20000: Avg Running Loss = 5.306424119472504\n",
            "Saving Checkpoint for step :  1700\n",
            "0 - Target captions:\n",
            " A boy catches a tennis ball with his racket. <|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a basketball player shooting a ball into a basket. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A filed covered in flowers next to a  clock tower.  \n",
            "1 - predicted_captions:\n",
            " A man is a man walking on a street. a. a. a. a. a<|endoftext|> \n",
            "Step 1700/20000: Avg Running Loss = 5.412612571716308\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  1800\n",
            "0 - Target captions:\n",
            " A woman with a red checkered shirt with towel around her neck.  \n",
            "0 - predicted_captions:\n",
            " A woman is a man and a woman sitting on a couch. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A clean small kitchen area with white furniture.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A kitchen with a refrigerator and a microwave. a. a. a. a. a.<|endoftext|> \n",
            "Step 1800/20000: Avg Running Loss = 5.33126371383667\n",
            "Saving Checkpoint for step :  1900\n",
            "0 - Target captions:\n",
            " A little girl sitting on a chair in a nursury  \n",
            "0 - predicted_captions:\n",
            " A man is a man a man a man a man a man a man a man a man<|endoftext|> \n",
            "1 - Target captions:\n",
            " Large building near roadway in large city environment.<|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is a man walking on a street. a street. a street. a street.<|endoftext|> \n",
            "Step 1900/20000: Avg Running Loss = 5.399075951576233\n",
            "Saving Checkpoint for step :  2000\n",
            "0 - Target captions:\n",
            " A family sitting at a table eating a meal <|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman is a woman holding a plate of food. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " Several people at a restaurant table by window eating.   \n",
            "1 - predicted_captions:\n",
            " A woman is a woman standing on a street. a. a. a. a. a<|endoftext|> \n",
            "Step 2000/20000: Avg Running Loss = 5.281699261665344\n",
            "Saving Checkpoint for step :  2100\n",
            "0 - Target captions:\n",
            " A living room with a long curved sofa photographed from above.  \n",
            "0 - predicted_captions:\n",
            " A kitchen with a table and a. a. a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " There are three monitors and one laptop out on the desk.  \n",
            "1 - predicted_captions:\n",
            " A woman is a woman sitting on a a a a a a a a a a a a<|endoftext|> \n",
            "Step 2100/20000: Avg Running Loss = 5.255696086883545\n",
            "Saving Checkpoint for step :  2200\n",
            "0 - Target captions:\n",
            " Black and white photograph of kids sitting around a table with food.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a man eating a sandwich on a table. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A female, with a purse beside her, sitting, waiting for a bus.  \n",
            "1 - predicted_captions:\n",
            " A man is a man walking on a street. a. street. a. a. a<|endoftext|> \n",
            "Step 2200/20000: Avg Running Loss = 5.11611757516861\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  2300\n",
            "0 - Target captions:\n",
            " A little boy is doing an art project on a desk.  \n",
            "0 - predicted_captions:\n",
            " A man is a man holding a phone and a a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A blue and white vase holding red and yellow flowers.  \n",
            "1 - predicted_captions:\n",
            " A woman is a woman sitting on a a a a a a a a a a a a<|endoftext|> \n",
            "Step 2300/20000: Avg Running Loss = 5.3635918188095095\n",
            "Saving Checkpoint for step :  2400\n",
            "0 - Target captions:\n",
            " a woman plays with a litte kids hair in an aiplane  \n",
            "0 - predicted_captions:\n",
            " A man is a man a a a a a a a a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " two girls are smiling while having some soup<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A woman is a man and a woman in a. a. a. a. a.<|endoftext|> \n",
            "Step 2400/20000: Avg Running Loss = 5.187004346847534\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  2500\n",
            "0 - Target captions:\n",
            " A vase filled with plants and purple flowers.<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman is a woman standing in front of a a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " cooking dinner on stove in small kitchen at home\n",
            "  \n",
            "1 - predicted_captions:\n",
            " A kitchen with a table and a refrigerator. a. a. a. a. a.<|endoftext|> \n",
            "Step 2500/20000: Avg Running Loss = 5.228288311958313\n",
            "Saving Checkpoint for step :  2600\n",
            "0 - Target captions:\n",
            " A woman on a cell phone and a single pink rose.  \n",
            "0 - predicted_captions:\n",
            " A man is a man standing on a a a. a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " The are multiple handheld phones in the picture.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is a sitting on a a a a a a a a a a a a a<|endoftext|> \n",
            "Step 2600/20000: Avg Running Loss = 5.237522821426392\n",
            "Saving Checkpoint for step :  2700\n",
            "0 - Target captions:\n",
            " A table with people working on laptop computers.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a man holding a phone and a woman a woman holding a phone. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A collection of books and a pocket watch in dim lighting.  \n",
            "1 - predicted_captions:\n",
            " A woman is a woman holding a bag of food on a table. a. a. a<|endoftext|> \n",
            "Step 2700/20000: Avg Running Loss = 5.165668983459472\n",
            "Saving Checkpoint for step :  2800\n",
            "0 - Target captions:\n",
            " Woman in casual attire playing a game of tennis.  \n",
            "0 - predicted_captions:\n",
            " A tennis player is a playing a tennis ball on a court. a.....<|endoftext|> \n",
            "1 - Target captions:\n",
            " a boy is swinging a baseball bat at a game  \n",
            "1 - predicted_captions:\n",
            " A young boy is a baseball player on the field. a. a. a. a.<|endoftext|> \n",
            "Step 2800/20000: Avg Running Loss = 5.238925144672394\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  2900\n",
            "0 - Target captions:\n",
            " A large teddy bear sits on a chair<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A girl is a girl holding a cup of ice cream a. a. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man and woman are siting on the couch playing the wii.  \n",
            "1 - predicted_captions:\n",
            " A girl is a girl sitting on a a a a a a a a a a a a<|endoftext|> \n",
            "Step 2900/20000: Avg Running Loss = 5.183397030830383\n",
            "Saving Checkpoint for step :  3000\n",
            "0 - Target captions:\n",
            " Plates of food and drinks on top of a table.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A plate of a a a a a a a a a a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man is taking a photo with his cell phone in front of a construction site.  \n",
            "1 - predicted_captions:\n",
            " A man is a man walking a dog on a street. a......<|endoftext|> \n",
            "Step 3000/20000: Avg Running Loss = 5.2043821144104\n",
            "Saving Checkpoint for step :  3100\n",
            "0 - Target captions:\n",
            " A variety of potted plats in a variety of sizes.<|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a man walking down a street with a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man in a suit and tie holding a beer next to a wall clock.  \n",
            "1 - predicted_captions:\n",
            " A man is a man standing on a a a a. a a. a. a.<|endoftext|> \n",
            "Step 3100/20000: Avg Running Loss = 5.104543678760528\n",
            "Saving Checkpoint for step :  3200\n",
            "0 - Target captions:\n",
            " A green vase is filled with tall flowers. <|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A woman is a woman holding a bouquet of flowers in her hand. a. a.<|endoftext|> \n",
            "1 - Target captions:\n",
            " Two ladies are on their phones, sitting at a red bench.   \n",
            "1 - predicted_captions:\n",
            " A man is a man with a dog on a bench. a. a. a. a<|endoftext|> \n",
            "Step 3200/20000: Avg Running Loss = 5.180781970024109\n",
            "Saving Checkpoint for step :  3300\n",
            "0 - Target captions:\n",
            " Many people walk in and out of a station. <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a man walking on a street. a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " People standing in a kitchen next to a table that has pizzas on it.  \n",
            "1 - predicted_captions:\n",
            " A man eating a sandwich on a table. a. a. a. a. a.<|endoftext|> \n",
            "Step 3300/20000: Avg Running Loss = 5.05670886516571\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  3400\n",
            "0 - Target captions:\n",
            " A plastic doll sitting on a desk next to an open lap top and a pair of scissors.  \n",
            "0 - predicted_captions:\n",
            " A girl is sitting on a a a a a a a a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A large clay planter with a tree and purple flowers inside of it.<|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is a man on a. a. a. a. a. a. a<|endoftext|> \n",
            "Step 3400/20000: Avg Running Loss = 5.1710359835624695\n",
            "Saving Checkpoint for step :  3500\n",
            "0 - Target captions:\n",
            " A man at a pizzeria with his order. <|endoftext|><|endoftext|><|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a eating a pizza on a table a. a. a. a. a<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man standing on a trolley loading or unloading a refrigerator.  \n",
            "1 - predicted_captions:\n",
            " A man is a man riding a bike a. a. a. a. a. a<|endoftext|> \n",
            "Step 3500/20000: Avg Running Loss = 5.102943117618561\n",
            "Saving Checkpoint for step :  3600\n",
            "0 - Target captions:\n",
            " Flower vase shaped like three attached, white guns.  \n",
            "0 - predicted_captions:\n",
            " A girl is a a a a a a a a a a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " He just wants to have fun with his toothbrush.<|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A child is a boy playing with a toy. a. a. a. a. a<|endoftext|> \n",
            "Step 3600/20000: Avg Running Loss = 5.129070115089417\n",
            "Saving Checkpoint for step :  3700\n",
            "0 - Target captions:\n",
            " Several people sitting next to one another in public transportation.<|endoftext|>  \n",
            "0 - predicted_captions:\n",
            " A man is a man walking down a street. a........<|endoftext|> \n",
            "1 - Target captions:\n",
            " A couple of people on a court with a tennis racket.  \n",
            "1 - predicted_captions:\n",
            " A player is playing tennis on a court...........<|endoftext|> \n",
            "Step 3700/20000: Avg Running Loss = 5.099883825778961\n",
            "Batch skipped as captions too long.\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  3800\n",
            "0 - Target captions:\n",
            " a table with chips, salsa, and mexican food with silverware and a beverage  \n",
            "0 - predicted_captions:\n",
            " A man eating a sandwich on a table...........<|endoftext|> \n",
            "1 - Target captions:\n",
            " An on microwave unit is heating something in a cup.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is a bus stop. a. a. a. a. a. a.<|endoftext|> \n",
            "Step 3800/20000: Avg Running Loss = 5.149784898757934\n",
            "Batch skipped as captions too long.\n",
            "Saving Checkpoint for step :  3900\n",
            "0 - Target captions:\n",
            " Two stuffed bears around a table with a bottle of alcohol and two shot glasses.  \n",
            "0 - predicted_captions:\n",
            " A man is sitting on a couch with a laptop on a table. a....<|endoftext|> \n",
            "1 - Target captions:\n",
            " Two computer mouses are on a silver pad.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A person is a person sitting on a a a a a a a a a a a a<|endoftext|> \n",
            "Step 3900/20000: Avg Running Loss = 5.110240936279297\n",
            "Saving Checkpoint for step :  4000\n",
            "0 - Target captions:\n",
            " A bowl that has a bunch of cobs of corn.  \n",
            "0 - predicted_captions:\n",
            " A man eating a sandwich on a table. a. a. a.....<|endoftext|> \n",
            "1 - Target captions:\n",
            " A man is swinging a bat in a grassy field.  \n",
            "1 - predicted_captions:\n",
            " A boy is playing a video game on a console. a. a.....<|endoftext|> \n",
            "Step 4000/20000: Avg Running Loss = 5.03709112405777\n",
            "Saving Checkpoint for step :  4100\n",
            "0 - Target captions:\n",
            " A man points a gun that goes to a video game.  \n",
            "0 - predicted_captions:\n",
            " A man is sitting on a a a a a a a a a a a a a a<|endoftext|> \n",
            "1 - Target captions:\n",
            " An old fashion oven is shown in  dim lighting.<|endoftext|>  \n",
            "1 - predicted_captions:\n",
            " A man is standing in front of a kitchen with a microwave. a.....<|endoftext|> \n",
            "Step 4100/20000: Avg Running Loss = 4.939563417434693\n"
          ]
        }
      ],
      "source": [
        "# Initialize Weights & Biases (WandB) for Experiment Tracking:\n",
        "wandb.init(project=\"clip_phi2_project\", name=\"clip_phi3_finetune\")\n",
        "# enables Automatic Mixed Precision (AMP) in PyTorch, specifically for CUDA-enabled GPUs.\n",
        "# Mixed Precision refers to using both 16-bit and 32-bit floating-point types during training.\n",
        "torch.amp.autocast('cuda', enabled=True)\n",
        "# torch.cuda.empty_cache(): This function frees up unused memory held by PyTorch in the CUDA memory cache.\n",
        "torch.cuda.empty_cache()\n",
        "# clears up any unreferenced memory\n",
        "gc.collect()\n",
        "#  This controls the precision used for 32-bit floating-point matrix multiplication\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}